{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "r2d2_hw5 (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9wp38iEgQJa"
      },
      "source": [
        "**In this homework, you will implement several AI models to conduct the intent detection task.**\n",
        "![alt text](https://i.ibb.co/fXmYHRq/ec5.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NXGtWRp1Xfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "457bf63a-95d8-42c6-8850-17e6a7f82e94"
      },
      "source": [
        "no = [\"Hello.\",\"What's up?\",\"What is your name?\",\"How old are you?\",\"Where are you from?\", \"How is the weather today?\",\"Do you like Monday?\",\"Hi.\",\"Hey all.\",\"Hello there.\"]\n",
        "print(len(set(no)))\n",
        "my_driving_sentences=[\"Go straight.\",\"Go left.\",\"Go right.\",\"Go back.\",\"Go south.\", \"Go north.\",\"Go west.\",\"Go east.\",\"Go forward for one feet.\",\"Go back for one feet.\"]\n",
        "print(len(set(my_driving_sentences)))\n",
        "my_light_sentences = [\"Turn off your lights.\",\"Turn on your lights.\",\"Change the back LED to green.\",\"Change the back LED to red.\",\"Turn off your holoemitter.\",\"Turn on your holoemitter.\",\"Blink your holoemitter.\",\"Blink all your lights.\",\"Turn on your logic display\",\"Sleep\"]\n",
        "print(len(set(my_light_sentences)))\n",
        "my_head_sentences = [\"Look ahead.\",\"Turn your head back.\",\"Turn your head left.\",\"Turn your head right.\",\"Look behind you.\",\"Turn your head to face south.\", \"Turn your head to face north.\",\"Turn your head to face west.\",\"Turn your head to face east.\",\"Spin your head around.\" ]\n",
        "print(len(set(my_head_sentences)))\n",
        "my_state_sentences = [\"What direction is your head facing?\",\"Is your holoemitter on?\",\"Is your logic display turn on?\",\"Is any of your lights on?\",\"Are all your lights off?\", \"What is your orientation?\",\"Where are you heading?\",\"Where does your head facing?\",\"Do any of your lights blink?\",\"What color is your back light?\"]\n",
        "print(len(set(my_state_sentences)))\n",
        "my_connection_sentences =  [\"Connect to server.\",\"Disconnect from the server.\",\"Is any droids nearby?\",\"Connect to droid nearby.\",\"Connect.\",\"Disconnect.\",\"Connect to anyone.\",\"Disconnect from everyone.\",\"Connection on.\",\"Connection off.\"]\n",
        "print(len(set(my_connection_sentences)))\n",
        "my_stance_sentences= [\"Put your third wheel up.\",\"Put your third wheel down.\",\"Set stance to be biped.\",\"Walk on two.\",\"Walk on three.\",\"Walk like human.\",\"Walk like robot.\",\"Stand on tiptoes.\",\"Stand normally.\",\"Set stance to default.\"]\n",
        "print(len(set(my_stance_sentences)))\n",
        "my_animation_sentences = [\"Fall.\",\"Make noise.\",\"Laugh.\",\"Be quiet.\",\"Shout.\",\"Scream.\",\"Fall over.\",\"Cry.\",\"Be loud.\",\"Play a song.\"]\n",
        "print(len(set(my_animation_sentences)))\n",
        "my_grid_sentences = [\"You are on a 3 by 3 grid.\", \"You are on a 4 by 4 grid.\",\"Go to position 0,0.\",\"Each square is 1 foot large.\",\"You are on a 2 by 2 grid.\",\"Go to the origin.\",\"Go to the right of the object.\",\"Go to the left of the object.\",\"There is a chair on your right.\",\"There is a chair on your left.\" ] \n",
        "print(len(set(my_grid_sentences)))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgSA_YXpHwFI"
      },
      "source": [
        "# Part 0: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuPUTB25ghjz"
      },
      "source": [
        "In this section, you will have a general idea of how the data looks like and do some simple transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKbddkNKfmNE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5893048-6b27-425f-e4cf-e3793eb1cd65"
      },
      "source": [
        "# download the data\n",
        "!wget \"https://drive.google.com/uc?export=download&id=1dLUN9oSB4u27NOleYE-Uksoh6RNQlZbi\" -O sample.p"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-25 02:51:30--  https://drive.google.com/uc?export=download&id=1dLUN9oSB4u27NOleYE-Uksoh6RNQlZbi\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.203.113, 173.194.203.138, 173.194.203.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.203.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/fervh7le4psud8i3hbdskb9mv7qu03r6/1608864675000/15787019596848476183/*/1dLUN9oSB4u27NOleYE-Uksoh6RNQlZbi?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-12-25 02:51:30--  https://doc-10-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/fervh7le4psud8i3hbdskb9mv7qu03r6/1608864675000/15787019596848476183/*/1dLUN9oSB4u27NOleYE-Uksoh6RNQlZbi?e=download\n",
            "Resolving doc-10-a0-docs.googleusercontent.com (doc-10-a0-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-10-a0-docs.googleusercontent.com (doc-10-a0-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 285436 (279K) [text/x-pascal]\n",
            "Saving to: ‘sample.p’\n",
            "\n",
            "sample.p            100%[===================>] 278.75K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2020-12-25 02:51:30 (116 MB/s) - ‘sample.p’ saved [285436/285436]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4at6WgHmCra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff5754e-6029-4cec-8c08-555cc23150de"
      },
      "source": [
        "# test sentences for evaluation\n",
        "!wget \"https://drive.google.com/uc?export=download&id=1gEW_qY5x8uPAhriiobubheYo6FC35btQ\" -O test_sentences.p"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-25 02:51:32--  https://drive.google.com/uc?export=download&id=1gEW_qY5x8uPAhriiobubheYo6FC35btQ\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.195.138, 74.125.195.100, 74.125.195.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.195.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0o-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/rr6g8qmu16s97gvdpb32lhb2h830hfg1/1608864675000/15787019596848476183/*/1gEW_qY5x8uPAhriiobubheYo6FC35btQ?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-12-25 02:51:33--  https://doc-0o-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/rr6g8qmu16s97gvdpb32lhb2h830hfg1/1608864675000/15787019596848476183/*/1gEW_qY5x8uPAhriiobubheYo6FC35btQ?e=download\n",
            "Resolving doc-0o-a0-docs.googleusercontent.com (doc-0o-a0-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-0o-a0-docs.googleusercontent.com (doc-0o-a0-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52258 (51K) [text/x-pascal]\n",
            "Saving to: ‘test_sentences.p’\n",
            "\n",
            "test_sentences.p    100%[===================>]  51.03K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2020-12-25 02:51:33 (75.3 MB/s) - ‘test_sentences.p’ saved [52258/52258]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYqcSyQsmgf1"
      },
      "source": [
        "import pickle\n",
        "samples = pickle.load(open(\"sample.p\", \"rb\"))\n",
        "test_sentences = pickle.load(open(\"test_sentences.p\", \"rb\"))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AicwRkV-mzqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "886f9f9c-e911-4b2b-d85e-d82124e204a5"
      },
      "source": [
        "###data structure###\n",
        "### [[sentence, label]] ###\n",
        "print(samples[:3])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Turn off the holoemitter.', 2], ['Halt.', 1], ['Get off tiptoes', 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1m_-5m2eymC"
      },
      "source": [
        "There are nine categories for these sentences, which are 'no', 'driving', 'light', 'head', 'state', 'connection', 'stance', 'animation' and 'grid'. The mapping from index to category name are shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YapxZx0sWhDg"
      },
      "source": [
        "ind2cat = {0: 'no', 1: 'driving', 2: 'light', 3: 'head', 4: 'state', 5: 'connection', 6: 'stance', 7: 'animation', 8: 'grid'}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1hHO6pInCH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9879dfc1-f181-4a28-bd15-137d3af1ec32"
      },
      "source": [
        "### Distribution on categories ###\n",
        "cat2sentence = {}\n",
        "for sample in samples:\n",
        "  sentence = sample[0]\n",
        "  cat = ind2cat[sample[1]]\n",
        "  if cat not in cat2sentence:\n",
        "    cat2sentence[cat] = [sentence]\n",
        "  else:\n",
        "    cat2sentence[cat].append(sentence)\n",
        "\n",
        "print(\"number of sentences for each category\")\n",
        "for cat, sentences in cat2sentence.items():\n",
        "  print(cat, \": \", len(sentences))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of sentences for each category\n",
            "light :  716\n",
            "driving :  784\n",
            "stance :  758\n",
            "head :  698\n",
            "grid :  678\n",
            "state :  676\n",
            "animation :  645\n",
            "no :  629\n",
            "connection :  673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blzu6p9hoJgi"
      },
      "source": [
        "### Train/Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReEMaskjoMZt"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "SENTENCES = [sample[0] for sample in samples]\n",
        "LABELS = [sample[1] for sample in samples]\n",
        "X_train, X_val, y_train, y_val = train_test_split(SENTENCES, LABELS, test_size=0.2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hh5RkEiHrt8"
      },
      "source": [
        "### Clean Text\n",
        "Write a tokenization function clean(sentence) which takes as input a string of text and returns a list of tokens derived from that text. Here, we define a token to be a contiguous sequence of non-whitespace characters. We will remove punctuation marks and convert the text to lowercase. Hint: Use the built-in constant string.punctuation, found in the string module, and/or python's regex library, re."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM0rg6vdHmxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "096e3c74-2607-4b03-a39c-55ab9e9259f0"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = stopwords.words('english')\n",
        "\n",
        "def clean(sentence):\n",
        "  '''1. tokenize the sentence (remove punctuation)\n",
        "     2. remove the stop words\n",
        "     3. convert all words to lowercase'''\n",
        "  sentence = re.sub(r\"[^a-zA-Z0-9 ]+\", r\" \", sentence)\n",
        "  sentence = sentence.lower().strip()\n",
        "  sentence = re.sub(r\"\\s+\", r\" \", sentence)\n",
        "  sentence = sentence.split()\n",
        "  return sentence\n",
        "\n",
        "X_train_token = [clean(sentence) for sentence in X_train]\n",
        "X_val_token = [clean(sentence) for sentence in X_val]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRb6Z7qBNgsm"
      },
      "source": [
        "max_len = max([len(i) for i in X_train_token + X_val_token])# Find the maximum length of tokens in train/val"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69ZXEDPX7S17",
        "outputId": "19bdbd65-8569-4540-8eaf-0e692be3d937"
      },
      "source": [
        "max_len"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CskkruUAMMfI"
      },
      "source": [
        "### Build a Vocabulary\n",
        "Build a vocabulary to map each word to an index, you need to first find the unique words in train/val set.\n",
        "\n",
        "Once you build a vocabulary, it's better to save it to a file for future use. Because the vocabulary may change each time you run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAxoqREjMMCI"
      },
      "source": [
        "from collections import Counter\n",
        "word_count = {} # count the frequency of each word\n",
        "word2ind = {} # build your vocabulary\n",
        "all_words = []\n",
        "for i in X_train_token + X_val_token:\n",
        "    for j in i:\n",
        "        all_words.append(j)\n",
        "word_count = dict(Counter(all_words).most_common())\n",
        "\n",
        "for ix, i in enumerate(sorted(word_count)):\n",
        "  word2ind[i] = ix\n",
        "  \n",
        "vocab_size = len(word2ind)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQuybRo1HqO-"
      },
      "source": [
        "# Part 1: Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opl1oMHFN0Ph"
      },
      "source": [
        "### Convert token to vector\n",
        "Convert each list of tokens into an array use the vocabulary you built before. The length of the vector is the max_len and remember to do zero-padding if a list's lenghth is smaller than max_len."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5zwLf-SOU0W"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize(tokens, max_len, word2ind):\n",
        "  '''\n",
        "  Input: list of tokens\n",
        "  Output: 1D numpy array (length = max_len)\n",
        "  '''\n",
        "  tmp = []\n",
        "  for token in tokens:\n",
        "    ind = word2ind.get(token,None)\n",
        "    if ind is not None:\n",
        "      tmp.append(ind)\n",
        "  tmp = tmp + [0]*(max_len - len(tmp))\n",
        "  return tmp\n",
        "\n",
        "X_train_array = np.array([vectorize(tokens, max_len, word2ind) for tokens in X_train_token])\n",
        "X_val_array = np.array([vectorize(tokens, max_len, word2ind) for tokens in X_val_token])\n",
        "assert X_train_array.shape[-1] == max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3kjj4ja8qfH",
        "outputId": "fe2df4b5-b3ef-48fb-80e7-9d211db03396"
      },
      "source": [
        "X_train_array.shape, len(X_train_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5005, 61), 5005)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATfeiT7IUftX"
      },
      "source": [
        "### One-hot label\n",
        "Convert the scalar label to 1D array (length = 9), e.g 0 -> array([1, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gru9TAqnUfYh"
      },
      "source": [
        "y_train_onehot = np.zeros((len(y_train), len(set(y_train))))\n",
        "for ix, i in enumerate(y_train):\n",
        "  y_train_onehot[ix][i] = 1\n",
        "\n",
        "y_val_onehot = np.zeros((len(y_val), len(set(y_train))))\n",
        "for ix, i in enumerate(y_val):\n",
        "  y_val_onehot[ix][i] = 1\n",
        "assert y_train_onehot.shape[1] == 9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsp0pgj6PI6A"
      },
      "source": [
        "### Build the Recurrent Neural Network\n",
        "Now it's time to build the RNN network to do the classification task, you could just refer to this [official document](https://www.tensorflow.org/guide/keras/rnn).\n",
        "\n",
        "You will need the Embedding layer, RNN layer and Dense layer, your last layer should project to the number of labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i02o0YSODZLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e15502f-e801-4b8f-bb96-041be3e72aa7"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f7248a10898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ODSzrR2RbbR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d16f1128-64f1-4c16-a99e-14af90075bd0"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential()\n",
        "# Embedding Layer, Input Dimension = vocab_size, Output Dimension = 64\n",
        "model.add(layers.Embedding(input_dim=vocab_size, output_dim=64))\n",
        "# Two LSTM layers with 64 Units\n",
        "model.add(layers.LSTM(64, return_sequences=True))\n",
        "model.add(layers.LSTM(64))\n",
        "# Dense to the number of classes with softmax activation function\n",
        "model.add(layers.Dense(len(set(y_train)), activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, None, 64)          93888     \n",
            "_________________________________________________________________\n",
            "lstm_22 (LSTM)               (None, None, 64)          33024     \n",
            "_________________________________________________________________\n",
            "lstm_23 (LSTM)               (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 9)                 585       \n",
            "=================================================================\n",
            "Total params: 160,521\n",
            "Trainable params: 160,521\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oLUl6dVYTIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e20bda4-8e3d-400e-98cf-98457aad0455"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(lr=0.01)\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max', patience=5)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(X_train_array, y_train_onehot, batch_size=64, epochs=40,validation_data=(X_val_array, y_val_onehot),callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "79/79 [==============================] - 4s 26ms/step - loss: 2.2016 - accuracy: 0.1135 - val_loss: 1.9819 - val_accuracy: 0.1941\n",
            "Epoch 2/40\n",
            "79/79 [==============================] - 1s 13ms/step - loss: 1.8601 - accuracy: 0.2221 - val_loss: 1.7586 - val_accuracy: 0.2412\n",
            "Epoch 3/40\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 1.3685 - accuracy: 0.4162 - val_loss: 1.2491 - val_accuracy: 0.4744\n",
            "Epoch 4/40\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 1.1151 - accuracy: 0.5199 - val_loss: 0.9065 - val_accuracy: 0.6773\n",
            "Epoch 5/40\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.8131 - accuracy: 0.6885 - val_loss: 0.7099 - val_accuracy: 0.7692\n",
            "Epoch 6/40\n",
            "79/79 [==============================] - 1s 11ms/step - loss: 0.9517 - accuracy: 0.6014 - val_loss: 0.6905 - val_accuracy: 0.8035\n",
            "Epoch 7/40\n",
            "79/79 [==============================] - 1s 11ms/step - loss: 0.6729 - accuracy: 0.7525 - val_loss: 0.6697 - val_accuracy: 0.8155\n",
            "Epoch 8/40\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.9605 - accuracy: 0.6683 - val_loss: 1.9570 - val_accuracy: 0.2316\n",
            "Epoch 9/40\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 1.6913 - accuracy: 0.3170 - val_loss: 0.8957 - val_accuracy: 0.6997\n",
            "Epoch 10/40\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.7354 - accuracy: 0.7468 - val_loss: 0.5025 - val_accuracy: 0.8371\n",
            "Epoch 11/40\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.3793 - accuracy: 0.8709 - val_loss: 0.3478 - val_accuracy: 0.8954\n",
            "Epoch 12/40\n",
            "79/79 [==============================] - 1s 11ms/step - loss: 0.2674 - accuracy: 0.9198 - val_loss: 0.2768 - val_accuracy: 0.9265\n",
            "Epoch 13/40\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.1831 - accuracy: 0.9439 - val_loss: 0.2447 - val_accuracy: 0.9393\n",
            "Epoch 14/40\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.1269 - accuracy: 0.9661 - val_loss: 0.2201 - val_accuracy: 0.9449\n",
            "Epoch 15/40\n",
            "79/79 [==============================] - 1s 13ms/step - loss: 0.1107 - accuracy: 0.9688 - val_loss: 0.2014 - val_accuracy: 0.9553\n",
            "Epoch 16/40\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.0958 - accuracy: 0.9733 - val_loss: 0.2043 - val_accuracy: 0.9569\n",
            "Epoch 17/40\n",
            "79/79 [==============================] - 1s 13ms/step - loss: 0.0665 - accuracy: 0.9801 - val_loss: 0.2133 - val_accuracy: 0.9577\n",
            "Epoch 18/40\n",
            "79/79 [==============================] - 1s 13ms/step - loss: 0.0824 - accuracy: 0.9732 - val_loss: 0.2420 - val_accuracy: 0.9425\n",
            "Epoch 19/40\n",
            "79/79 [==============================] - 1s 13ms/step - loss: 0.0901 - accuracy: 0.9763 - val_loss: 0.1759 - val_accuracy: 0.9585\n",
            "Epoch 20/40\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.0766 - accuracy: 0.9779 - val_loss: 0.1899 - val_accuracy: 0.9609\n",
            "Epoch 21/40\n",
            "79/79 [==============================] - 1s 11ms/step - loss: 0.0543 - accuracy: 0.9843 - val_loss: 0.1651 - val_accuracy: 0.9609\n",
            "Epoch 22/40\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.0888 - accuracy: 0.9728 - val_loss: 0.1988 - val_accuracy: 0.9569\n",
            "Epoch 23/40\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.0552 - accuracy: 0.9844 - val_loss: 0.1726 - val_accuracy: 0.9633\n",
            "Epoch 24/40\n",
            "79/79 [==============================] - 1s 11ms/step - loss: 0.0348 - accuracy: 0.9908 - val_loss: 0.1772 - val_accuracy: 0.9593\n",
            "Epoch 25/40\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.0301 - accuracy: 0.9898 - val_loss: 0.1753 - val_accuracy: 0.9617\n",
            "Epoch 26/40\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.0219 - accuracy: 0.9910 - val_loss: 0.1813 - val_accuracy: 0.9593\n",
            "Epoch 27/40\n",
            "79/79 [==============================] - 1s 13ms/step - loss: 0.0271 - accuracy: 0.9902 - val_loss: 0.1804 - val_accuracy: 0.9609\n",
            "Epoch 28/40\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.0247 - accuracy: 0.9903 - val_loss: 0.1739 - val_accuracy: 0.9609\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f716e841198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH1Xe7EMlITl"
      },
      "source": [
        "### Evaluate on the test sentences\n",
        "Now run your model to predict on the test sentences, you need to do the preprocessing on these sentences first and save your prediction to a list of labels, e.g [0, 2, 1, 5, ....]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBKwqNUElnyu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "outputId": "af9f02e0-1c8e-435c-c462-5f5444e2d75f"
      },
      "source": [
        "test_prediction = []\n",
        "#TODO\n",
        "print(len(set(test_sentences)))\n",
        "test_sentences = [clean(i) for i in test_sentences]\n",
        "all_test_vectors = np.array([vectorize(tokens, max_len, word2ind) for tokens in test_sentences])\n",
        "test_prediction = model.predict(all_test_vectors)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1198\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-8900720d801d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_test_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2ind\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_test_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1434 predict_step\n        return self(x, training=False)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:223 assert_input_compatibility\n        str(tuple(shape)))\n\n    ValueError: Input 0 of layer sequential_17 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 61)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqZbanaSET4K"
      },
      "source": [
        "test_prediction = test_prediction.argmax(-1)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4wIUAjxEh8F",
        "outputId": "2130ebd1-5196-45a6-bb4e-cca1daa2c64d"
      },
      "source": [
        "test_prediction\n"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 1, 7, ..., 3, 2, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbZmj4Dolo0-"
      },
      "source": [
        "# Save the results and upload to Gradescope\n",
        "pickle.dump(test_prediction, open(\"rnn.p\", \"wb\"))"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8v6GBKws0Z7"
      },
      "source": [
        "#Part 2. Word Embedding via pymagnitude\n",
        "Instead of using the vocabulary to convert word to number, you could use pretrained word embeddings to do the task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbMVzcGDucgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b5831e4-1ddb-4b0f-9a6b-cda405aea3da"
      },
      "source": [
        "! echo \"Installing Magnitude.... (please wait, can take a while)\"\n",
        "! (curl https://raw.githubusercontent.com/plasticityai/magnitude/master/install-colab.sh | /bin/bash 1>/dev/null 2>/dev/null)\n",
        "! echo \"Done installing Magnitude.\""
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing Magnitude.... (please wait, can take a while)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   137  100   137    0     0    867      0 --:--:-- --:--:-- --:--:--   867\n",
            "Done installing Magnitude.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VzKjJNNxkYH"
      },
      "source": [
        "Next, you'll need to download a pre-trained set of word embeddings. We'll get a set trained with Google's word2vec algorithm, which we discussed in class. [Here](https://gitlab.com/Plasticity/magnitude), you can check the full list of available embeddings, feel free to try different embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfDkasoHxLjD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c93e9257-7d23-448b-e145-ec91d51ddf65"
      },
      "source": [
        "# Download Pretrained Word-Embedding\n",
        "! wget http://magnitude.plasticity.ai/word2vec/light/GoogleNews-vectors-negative300.magnitude"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-25 02:27:20--  http://magnitude.plasticity.ai/word2vec/light/GoogleNews-vectors-negative300.magnitude\n",
            "Resolving magnitude.plasticity.ai (magnitude.plasticity.ai)... 52.217.41.27\n",
            "Connecting to magnitude.plasticity.ai (magnitude.plasticity.ai)|52.217.41.27|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4211335168 (3.9G) [binary/octet-stream]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.magnitude’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   3.92G  47.2MB/s    in 78s     \n",
            "\n",
            "2020-12-25 02:28:38 (51.7 MB/s) - ‘GoogleNews-vectors-negative300.magnitude’ saved [4211335168/4211335168]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngFFKNj8yAU5"
      },
      "source": [
        "# Load the embedding\n",
        "from pymagnitude import *\n",
        "vectors = Magnitude(\"GoogleNews-vectors-negative300.magnitude\") \n",
        "D = vectors.query(\"cat\").shape[0]"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pdRsfIuVxIg"
      },
      "source": [
        "### Convert tokens to embeddings\n",
        "You could now use the pymagnitude to query each token and convert them to a list of embeddings. Note that you need to do zero padding to match the maximum length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0LMW9_OXjs-"
      },
      "source": [
        "def embedding(list_tokens, max_len, vectors, D):\n",
        "  '''\n",
        "  return an array with the shape (n_of_samples, max_len, D)\n",
        "  '''\n",
        "  lst = []\n",
        "  for x in list_tokens:\n",
        "    tmp = [vectors.query(i) for i in x][-max_len:]\n",
        "    tmp = tmp + [np.zeros(D)]*(max_len - len(tmp))\n",
        "    lst.append(np.asarray(tmp))\n",
        "  return np.asarray(lst)\n",
        "\n",
        "X_train_embedding = embedding(X_train_token, max_len, vectors, D)\n",
        "X_val_embedding = embedding(X_val_token, max_len, vectors, D)\n",
        "\n",
        "assert X_train_embedding.shape[-1] == D\n",
        "assert X_train_embedding.shape[-2] == max_len"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZjeg9csyNYS",
        "outputId": "2b647a7c-3ee9-4eb9-979b-cd27441d7b9e"
      },
      "source": [
        "max([len(i) for i in test_sentences])"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "76"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxoxUCB8YPul"
      },
      "source": [
        "### Build the RNN model\n",
        "Similar to Part 1, build a RNN model using your new embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER6xPrArYPLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7c4a0a-2551-4fbb-bf42-963dc0c23f81"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential()\n",
        "#TODO\n",
        "# LSTM Layer with input shape (max_len, D), output shape (max_len, 256)\n",
        "model.add(layers.LSTM(256, return_sequences=True, input_shape=(max_len, D)))\n",
        "# LSTM Layer with 128 units\n",
        "model.add(layers.LSTM(128))\n",
        "# Dense to 64 with tanh activation function\n",
        "model.add(layers.Dense(64, activation='tanh'))\n",
        "# Dense to number of classes with softmax function\n",
        "model.add(layers.Dense(len(set(y_train)), activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_34 (LSTM)               (None, 61, 256)           570368    \n",
            "_________________________________________________________________\n",
            "lstm_35 (LSTM)               (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 9)                 585       \n",
            "=================================================================\n",
            "Total params: 776,329\n",
            "Trainable params: 776,329\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDyV_5F2kwFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a45928-b32e-490a-bec8-47f315fdb763"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(lr=2e-4)\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max', patience=10)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(X_train_embedding, y_train_onehot, batch_size=64, epochs=40, validation_data=(X_val_embedding, y_val_onehot),callbacks=[callback])"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "79/79 [==============================] - 6s 32ms/step - loss: 2.1814 - accuracy: 0.1282 - val_loss: 1.9043 - val_accuracy: 0.2260\n",
            "Epoch 2/40\n",
            "79/79 [==============================] - 2s 20ms/step - loss: 1.8924 - accuracy: 0.2152 - val_loss: 1.6023 - val_accuracy: 0.3315\n",
            "Epoch 3/40\n",
            "79/79 [==============================] - 2s 20ms/step - loss: 1.5254 - accuracy: 0.3427 - val_loss: 1.3848 - val_accuracy: 0.4281\n",
            "Epoch 4/40\n",
            "79/79 [==============================] - 1s 18ms/step - loss: 1.3317 - accuracy: 0.4387 - val_loss: 1.3214 - val_accuracy: 0.4673\n",
            "Epoch 5/40\n",
            "79/79 [==============================] - 2s 19ms/step - loss: 1.2379 - accuracy: 0.4903 - val_loss: 1.2214 - val_accuracy: 0.4856\n",
            "Epoch 6/40\n",
            "79/79 [==============================] - 2s 21ms/step - loss: 1.0752 - accuracy: 0.5703 - val_loss: 0.9588 - val_accuracy: 0.6486\n",
            "Epoch 7/40\n",
            "79/79 [==============================] - 2s 21ms/step - loss: 1.0949 - accuracy: 0.5953 - val_loss: 1.0298 - val_accuracy: 0.6174\n",
            "Epoch 8/40\n",
            "79/79 [==============================] - 2s 21ms/step - loss: 0.8770 - accuracy: 0.7028 - val_loss: 0.7976 - val_accuracy: 0.7053\n",
            "Epoch 9/40\n",
            "79/79 [==============================] - 2s 21ms/step - loss: 0.7446 - accuracy: 0.7445 - val_loss: 0.8320 - val_accuracy: 0.7356\n",
            "Epoch 10/40\n",
            "79/79 [==============================] - 2s 20ms/step - loss: 0.7507 - accuracy: 0.7542 - val_loss: 0.7447 - val_accuracy: 0.7636\n",
            "Epoch 11/40\n",
            "79/79 [==============================] - 2s 19ms/step - loss: 0.6804 - accuracy: 0.7852 - val_loss: 0.6549 - val_accuracy: 0.7883\n",
            "Epoch 12/40\n",
            "79/79 [==============================] - 2s 21ms/step - loss: 0.6290 - accuracy: 0.7857 - val_loss: 0.6224 - val_accuracy: 0.8067\n",
            "Epoch 13/40\n",
            "79/79 [==============================] - 2s 26ms/step - loss: 0.6300 - accuracy: 0.7963 - val_loss: 0.9245 - val_accuracy: 0.6621\n",
            "Epoch 14/40\n",
            "79/79 [==============================] - 2s 20ms/step - loss: 0.7335 - accuracy: 0.7333 - val_loss: 0.7305 - val_accuracy: 0.7556\n",
            "Epoch 15/40\n",
            "79/79 [==============================] - 2s 21ms/step - loss: 0.7514 - accuracy: 0.7500 - val_loss: 0.7516 - val_accuracy: 0.7516\n",
            "Epoch 16/40\n",
            "79/79 [==============================] - 2s 19ms/step - loss: 0.6566 - accuracy: 0.7804 - val_loss: 0.6719 - val_accuracy: 0.7827\n",
            "Epoch 17/40\n",
            "79/79 [==============================] - 2s 21ms/step - loss: 0.6492 - accuracy: 0.7923 - val_loss: 0.6243 - val_accuracy: 0.7979\n",
            "Epoch 18/40\n",
            "79/79 [==============================] - 2s 22ms/step - loss: 0.5598 - accuracy: 0.8268 - val_loss: 0.5415 - val_accuracy: 0.8363\n",
            "Epoch 19/40\n",
            "79/79 [==============================] - 2s 21ms/step - loss: 0.5240 - accuracy: 0.8360 - val_loss: 0.6404 - val_accuracy: 0.7891\n",
            "Epoch 20/40\n",
            "79/79 [==============================] - 2s 21ms/step - loss: 0.5422 - accuracy: 0.8182 - val_loss: 0.5514 - val_accuracy: 0.8331\n",
            "Epoch 21/40\n",
            "79/79 [==============================] - 1s 18ms/step - loss: 0.4924 - accuracy: 0.8495 - val_loss: 0.5003 - val_accuracy: 0.8490\n",
            "Epoch 22/40\n",
            "79/79 [==============================] - 1s 19ms/step - loss: 0.4455 - accuracy: 0.8637 - val_loss: 0.5157 - val_accuracy: 0.8387\n",
            "Epoch 23/40\n",
            "79/79 [==============================] - 1s 19ms/step - loss: 0.4169 - accuracy: 0.8720 - val_loss: 0.4392 - val_accuracy: 0.8722\n",
            "Epoch 24/40\n",
            "79/79 [==============================] - 2s 20ms/step - loss: 0.4136 - accuracy: 0.8774 - val_loss: 0.4648 - val_accuracy: 0.8626\n",
            "Epoch 25/40\n",
            "79/79 [==============================] - 2s 23ms/step - loss: 0.3778 - accuracy: 0.8865 - val_loss: 0.5006 - val_accuracy: 0.8474\n",
            "Epoch 26/40\n",
            "79/79 [==============================] - 2s 22ms/step - loss: 0.3488 - accuracy: 0.8968 - val_loss: 0.4463 - val_accuracy: 0.8658\n",
            "Epoch 27/40\n",
            "79/79 [==============================] - 2s 19ms/step - loss: 0.3466 - accuracy: 0.8961 - val_loss: 0.4651 - val_accuracy: 0.8698\n",
            "Epoch 28/40\n",
            "79/79 [==============================] - 2s 20ms/step - loss: 0.3133 - accuracy: 0.9116 - val_loss: 0.4329 - val_accuracy: 0.8802\n",
            "Epoch 29/40\n",
            "79/79 [==============================] - 1s 18ms/step - loss: 0.2897 - accuracy: 0.9143 - val_loss: 0.4034 - val_accuracy: 0.8802\n",
            "Epoch 30/40\n",
            "79/79 [==============================] - 2s 20ms/step - loss: 0.2925 - accuracy: 0.9166 - val_loss: 0.4190 - val_accuracy: 0.8778\n",
            "Epoch 31/40\n",
            "79/79 [==============================] - 2s 21ms/step - loss: 0.2883 - accuracy: 0.9173 - val_loss: 0.4070 - val_accuracy: 0.8818\n",
            "Epoch 32/40\n",
            "79/79 [==============================] - 2s 22ms/step - loss: 0.3151 - accuracy: 0.9067 - val_loss: 0.4014 - val_accuracy: 0.8778\n",
            "Epoch 33/40\n",
            "79/79 [==============================] - 2s 21ms/step - loss: 0.2713 - accuracy: 0.9165 - val_loss: 0.3710 - val_accuracy: 0.8930\n",
            "Epoch 34/40\n",
            "79/79 [==============================] - 2s 22ms/step - loss: 0.2591 - accuracy: 0.9280 - val_loss: 0.4772 - val_accuracy: 0.8618\n",
            "Epoch 35/40\n",
            "79/79 [==============================] - 2s 20ms/step - loss: 0.2623 - accuracy: 0.9125 - val_loss: 0.3613 - val_accuracy: 0.8970\n",
            "Epoch 36/40\n",
            "79/79 [==============================] - 2s 19ms/step - loss: 0.2559 - accuracy: 0.9223 - val_loss: 0.4087 - val_accuracy: 0.8850\n",
            "Epoch 37/40\n",
            "79/79 [==============================] - 1s 19ms/step - loss: 0.2700 - accuracy: 0.9149 - val_loss: 0.3537 - val_accuracy: 0.9042\n",
            "Epoch 38/40\n",
            "79/79 [==============================] - 2s 21ms/step - loss: 0.2411 - accuracy: 0.9271 - val_loss: 0.3481 - val_accuracy: 0.9058\n",
            "Epoch 39/40\n",
            "79/79 [==============================] - 2s 22ms/step - loss: 0.2228 - accuracy: 0.9309 - val_loss: 0.3627 - val_accuracy: 0.8994\n",
            "Epoch 40/40\n",
            "79/79 [==============================] - 2s 22ms/step - loss: 0.4066 - accuracy: 0.8778 - val_loss: 0.4087 - val_accuracy: 0.8818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7063e87128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGEJ-FtZl3nN"
      },
      "source": [
        "### Evaluate on the test sentences\n",
        "Now run your model to predict on the test sentences, you need to do the preprocessing on these sentences first and save your prediction to a list of labels, e.g [0, 2, 1, 5, ....]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8g9CBNAl8Eo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66817b6f-0bfe-4fb4-f6d3-ffb1a103edab"
      },
      "source": [
        "test_prediction = []\n",
        "#TODO\n",
        "# test_sentences = pickle.load(open(\"test_sentences.p\", \"rb\"))\n",
        "print(len(test_sentences))\n",
        "# test_sentences = [clean(i) for i in test_sentences]\n",
        "all_test_vectors = embedding(test_sentences, max_len, vectors, D)\n",
        "test_prediction = model.predict(all_test_vectors)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSHvcpwWK1iw",
        "outputId": "a75bb233-553c-4412-ecd1-f69ee3fcc115"
      },
      "source": [
        "test_prediction = test_prediction.argmax(-1)\n",
        "test_prediction"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 1, 7, ..., 3, 2, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAp7kXXpl_L3"
      },
      "source": [
        "# Save the results and upload to Gradescope\n",
        "pickle.dump(test_prediction, open(\"embedding.p\", \"wb\"))"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9dazGvsOzs7"
      },
      "source": [
        "# Part 3: BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQZlpz9xMqFo"
      },
      "source": [
        "In this part, you will use the BERT pipeline to further improve the performance.\n",
        "\n",
        "This part is open-ended, we just provide one example of using BERT, feel free to find other tutorial online to customize on this task.\n",
        "\n",
        "[Here](https://huggingface.co/models) is the list of all existing models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuQDOJNpONp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06db7323-e852-48b7-f2de-f6251b6ff4a5"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 32.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 50.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=659de00008e9b5c1316ce8c873924fadd679aa1f8f9bfe3cdef3a37a269e8100\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n",
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (51.0.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt7_h3tkOili",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98fd38cc-cb7b-4bb8-b285-752f886adc27"
      },
      "source": [
        "import transformers\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig, TFBertForSequenceClassification\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") #feel free to change the model\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=9)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59drUhQZO7ep"
      },
      "source": [
        "### Use BERT Tokenizer to preprocess the data\n",
        "The BERT Tokenizer will return a dictionary which contains 'input_ids', 'token_type_ids' and 'attention_mask', we will use the 'input_ids' and 'attention_mask' later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HJrt4odOx-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ee6d27-977b-48a9-add8-8e799426df7e"
      },
      "source": [
        "# Test the tokenizer\n",
        "sent = X_train[0]\n",
        "tokenized_sequence= bert_tokenizer.encode_plus(sent,add_special_tokens = True,\n",
        "                                              max_length =30,pad_to_max_length = True, \n",
        "                                              return_attention_mask = True)\n",
        "print(tokenized_sequence)\n",
        "print(bert_tokenizer.decode(tokenized_sequence['input_ids']))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': [101, 2191, 2070, 4165, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
            "[CLS] make some sounds [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y1cnxanRD1d"
      },
      "source": [
        "Use the bert tokenizer described above, encode the training and validations sentences, note that the max length should be 64."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpBf_0Z_P4oz"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def BERT_Tokenizer(sentences):\n",
        "  '''Input: list of sentences\n",
        "     Output: two numpy array\n",
        "  '''\n",
        "  op = bert_tokenizer.batch_encode_plus(sentences, add_special_tokens=True, \n",
        "                                        max_length=64, padding='max_length', truncation=True)\n",
        "  return np.asarray(op['input_ids']), np.array(op['attention_mask'])\n",
        "\n",
        "X_train_ids, X_train_masks = BERT_Tokenizer(X_train)\n",
        "X_val_ids, X_val_masks = BERT_Tokenizer(X_val)\n",
        "y_train_array = np.array(y_train)\n",
        "y_val_array = np.array(y_val)\n",
        "assert X_train_ids.shape[-1] == 64"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MypXAdRKR0Cp"
      },
      "source": [
        "import tensorflow as tf\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08)\n",
        "bert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXA2UDc3SaAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d6b3ef-6674-49db-ddf9-98934024142f"
      },
      "source": [
        "bert_model.fit([X_train_ids,X_train_masks],y_train_array,batch_size=32,epochs=6,validation_data=([X_val_ids,X_val_masks],y_val_array))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - ETA: 0s - loss: 1.5712 - accuracy: 0.5670"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r157/157 [==============================] - 88s 471ms/step - loss: 1.5678 - accuracy: 0.5682 - val_loss: 0.3029 - val_accuracy: 0.9385\n",
            "Epoch 2/6\n",
            "157/157 [==============================] - 71s 453ms/step - loss: 0.2445 - accuracy: 0.9551 - val_loss: 0.1386 - val_accuracy: 0.9625\n",
            "Epoch 3/6\n",
            "157/157 [==============================] - 71s 451ms/step - loss: 0.1232 - accuracy: 0.9701 - val_loss: 0.1040 - val_accuracy: 0.9760\n",
            "Epoch 4/6\n",
            "157/157 [==============================] - 71s 454ms/step - loss: 0.0718 - accuracy: 0.9833 - val_loss: 0.1141 - val_accuracy: 0.9704\n",
            "Epoch 5/6\n",
            "157/157 [==============================] - 71s 452ms/step - loss: 0.0476 - accuracy: 0.9879 - val_loss: 0.0815 - val_accuracy: 0.9768\n",
            "Epoch 6/6\n",
            "157/157 [==============================] - 71s 452ms/step - loss: 0.0484 - accuracy: 0.9861 - val_loss: 0.1123 - val_accuracy: 0.9744\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1a0759f1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpWa8cyTXfV"
      },
      "source": [
        "### Evaluate on test sentences\n",
        "Again, use BERT to predict on the test sentences and submit to Gradescope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkNQDrOYTkf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40710105-0006-49f9-eba4-9a3d5fd2470a"
      },
      "source": [
        "test_prediction = []\n",
        "#TODO\n",
        "test_sentences = pickle.load(open(\"test_sentences.p\", \"rb\"))\n",
        "print(len(test_sentences))\n",
        "test_sentences = BERT_Tokenizer(test_sentences)\n",
        "test_prediction = bert_model.predict(test_sentences)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LteWK62uRjEI",
        "outputId": "db1cbee8-84d4-4fc0-da88-db3591236bf1"
      },
      "source": [
        "test_prediction = test_prediction[0].argmax(-1)\n",
        "test_prediction"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 1, 7, ..., 3, 2, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3MItfdrTp8C"
      },
      "source": [
        "pickle.dump(test_prediction, open(\"bert.p\", \"wb\"))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH1nb2_NrEv0"
      },
      "source": [
        "# Part 4: Write your own commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK3_t0qzrNqs"
      },
      "source": [
        "Please write 10 sentences for each category, this will be very helpful for future students!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b71YkD41rMrc"
      },
      "source": [
        "my_commands = {'no': [], \n",
        "               'driving': [], \n",
        "               'light': [],\n",
        "               'head': [],\n",
        "               'state': [],\n",
        "               'connection': [], \n",
        "               'stance': [], \n",
        "               'animation': [],\n",
        "               'grid': []}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAT6yk_Ar-aY"
      },
      "source": [
        "pickle.dump(my_commands, open(\"my_commands.p\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}